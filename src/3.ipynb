{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "from pyecharts.globals import ChartType\n",
    "# the first graph to show the distribution of selected McDonald's Store \n",
    "from pyecharts.charts import *\n",
    "from pyecharts import options as opts\n",
    "import pandas as pd\n",
    "from pyecharts.globals import ThemeType"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T04:49:45.370597446Z",
     "start_time": "2023-12-20T04:49:45.332435289Z"
    }
   },
   "id": "6696aaf31f45f8bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('../dataset/McDonald/McDonald_s_Reviews.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "312a3043073525b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import reverse_geocoder as rg\n",
    "\n",
    "def get_state_name(lat, lon):\n",
    "    result = rg.search((lat, lon))\n",
    "    \n",
    "    if result:\n",
    "        state_name = result[0]['admin1']\n",
    "        return state_name\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "# Example coordinates\n",
    "lat = 38.456085\n",
    "lon = -92.288368\n",
    "\n",
    "state_name = get_state_name(lat, lon)\n",
    "print(f\"The coordinates (lat={lat}, lon={lon}) are in the state of {state_name}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be4641ff1a42117f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('../dataset/McDonald/McDonald_s_Reviews.csv')\n",
    "# Define a function to clean the 'rating' column and convert to numeric\n",
    "def clean_rating(rating):\n",
    "    if rating == \"1 star\":\n",
    "        return float(rating.replace(' star', ''))\n",
    "    else:\n",
    "        return float(rating.replace(' stars', ''))\n",
    "\n",
    "# Apply the function to the 'rating' column\n",
    "data['rating'] = data['rating'].apply(clean_rating)\n",
    "\n",
    "# if the store address is the same but the rating_count is different, we will change the highest rating_count\n",
    "# data = data.sort_values(by=['store_address', 'rating_count'], ascending=False)\n",
    "# data = data.drop_duplicates(subset=['store_address'], keep='first')\n",
    "\n",
    "for i in range(len(data)):\n",
    "    # transform the 2,808  to 2808\n",
    "    data.loc[i, 'rating_count'] = data.loc[i, 'rating_count'].replace(',', '')\n",
    "    data.loc[i, 'rating_count'] = int(data.loc[i, 'rating_count'])\n",
    "    if data.loc[i, 'rating_count'] == 1306:\n",
    "        data.loc[i, 'rating_count'] = 1307\n",
    "    if data.loc[i, 'rating_count'] == 1794:\n",
    "        data.loc[i, 'rating_count'] = 1795\n",
    "    if data.loc[i, 'rating_count'] == 998:\n",
    "        data.loc[i, 'rating_count'] = 999\n",
    "    if data.loc[i, 'rating_count'] == 1617:\n",
    "        data.loc[i, 'rating_count'] = 1618\n",
    "    if data.loc[i, 'rating_count'] == 3380:\n",
    "        data.loc[i, 'rating_count'] = 3381\n",
    "    if data.loc[i, 'rating_count'] == 2808:\n",
    "        data.loc[i, 'rating_count'] = 2810\n",
    "    if data.loc[i, 'rating_count'] == 5566:\n",
    "        data.loc[i, 'rating_count'] = 5567\n",
    "    if data.loc[i, 'rating_count'] == 1564:\n",
    "        data.loc[i, 'rating_count'] = 1565\n",
    "    if data.loc[i, 'rating_count'] == 19671:\n",
    "        data.loc[i, 'rating_count'] = 19682\n",
    "    if data.loc[i, 'rating_count'] == 5466:\n",
    "        data.loc[i, 'rating_count'] = 5468\n",
    "\n",
    "# Group by 'store_address' and calculate the mean rating\n",
    "average_ratings = data.groupby(['store_address','latitude', 'longitude','rating_count'])['rating'].mean()\n",
    "\n",
    "\n",
    "# add the state name to the data\n",
    "average_ratings = average_ratings.reset_index()\n",
    "\n",
    "average_ratings['state'] = average_ratings.apply(lambda row: get_state_name(row['latitude'], row['longitude']), axis=1)\n",
    "data_unique = average_ratings[['store_address', 'latitude', 'longitude','rating','state',\"rating_count\"]]\n",
    "\n",
    "print(data_unique)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d566721daf698a59"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/ByteBloom/Visualization/res/lab3/parallel_interactive.html'"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize the Parallel chart\n",
    "parallel = Parallel(init_opts=opts.InitOpts(theme=ThemeType.LIGHT))\n",
    "\n",
    "# Prepare the parallel data\n",
    "parallel_data = [[i[0], i[4], i[5], i[3]] for i in data_unique.values]\n",
    "\n",
    "# for all the parallel data[0], we only need the str before the first comma\n",
    "for i in range(len(parallel_data)):\n",
    "    parallel_data[i][0] = parallel_data[i][0].split(',')[0]\n",
    "\n",
    "# Add schema (axes) to the parallel chart\n",
    "parallel.add_schema(\n",
    "    [\n",
    "        opts.ParallelAxisOpts(dim=0, \n",
    "                              name=\"store_address\", \n",
    "                              type_=\"category\",),\n",
    "        opts.ParallelAxisOpts(dim=1, \n",
    "                              name=\"state\",\n",
    "                              type_=\"category\",),\n",
    "            \n",
    "        {\"dim\": 2, \"name\": \"rating count\"},\n",
    "        {\"dim\": 3, \"name\": \"average rating\"},\n",
    "        \n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add the data to the parallel chart\n",
    "parallel.add(\n",
    "    \"McDonald's Store\",\n",
    "    parallel_data,\n",
    "    linestyle_opts=opts.LineStyleOpts(width=2, opacity=0.5),\n",
    ")\n",
    "\n",
    "# Add interactive elements\n",
    "parallel.set_global_opts(\n",
    "    title_opts=opts.TitleOpts(title=\"Parallel-Category\"),\n",
    "    toolbox_opts=opts.ToolboxOpts(),\n",
    "    tooltip_opts=opts.TooltipOpts(trigger=\"item\", formatter=\"{c}\"),\n",
    "    brush_opts=opts.BrushOpts(\n",
    "        x_axis_index=\"all\",\n",
    "        brush_link=\"all\",\n",
    "        out_of_brush={\"colorAlpha\": 0.1},\n",
    "        brush_type=\"lineX\",\n",
    "    ),\n",
    "    datazoom_opts=[\n",
    "        opts.DataZoomOpts(range_start=0, range_end=100, orient=\"vertical\")\n",
    "    ],\n",
    "    visualmap_opts=opts.VisualMapOpts(\n",
    "     max_= 4, min_= 2,is_piecewise=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Render the plot to a file\n",
    "output_file = \"../res/lab3/parallel_interactive.html\"\n",
    "parallel.render(output_file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T05:11:58.611041358Z",
     "start_time": "2023-12-20T05:11:58.595955666Z"
    }
   },
   "id": "79ddb522684ae478"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-73.4598199 44.9814099\n",
      "-118.4259996 34.0565932\n",
      "-118.3679036 34.1525074\n",
      "-97.2229254 33.0093179\n",
      "-87.7779131 41.8796564\n",
      "-117.2494697 32.7976607\n",
      "-81.4051029 28.3999863\n",
      "-73.9882797 40.7188385\n",
      "-97.7928744 30.4607176\n",
      "-81.3426916 28.6553497\n",
      "-73.9890961 40.750831\n",
      "-118.4945396 34.0122192\n",
      "-74.0099791 40.7093746\n",
      "-80.1320774 25.7902955\n",
      "-75.3999187 40.9220806\n",
      "-77.0442355 38.9054561\n",
      "-111.8973832 40.7590573\n",
      "-97.3076518 32.9580411\n",
      "-98.6345801 29.6762669\n",
      "-74.0011677 40.7185138\n",
      "-80.1890981 25.8099996\n",
      "-73.990583 40.7505058\n",
      "-73.9928762 40.7525294\n",
      "-118.2829548 33.9312613\n",
      "-121.9954211 37.3854949\n",
      "-77.0289132 38.8969797\n",
      "-81.5137384 28.3335081\n",
      "-117.1780115 32.8206541\n",
      "-74.1614754 40.7719101\n",
      "-96.8122859 32.7445961\n",
      "-96.8122859 32.7445961\n",
      "-81.4714142 28.4503866\n",
      "-118.2382063 34.0352699\n",
      "-77.1850548 38.8128231\n",
      "-74.9810994 39.8520594\n",
      "-73.8124604 40.7274006\n",
      "-73.9932643 40.7291264\n",
      "-97.6606295 30.3291349\n",
      "-81.4612421 28.4238144\n",
      "-115.1758223 36.0902437\n"
     ]
    },
    {
     "data": {
      "text/plain": "'/home/ByteBloom/Visualization/res/lab3/mcdonalds_locations_geo.html'"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The GEO graph to show the distribution of selected McDonald's Store\n",
    "\n",
    "data_state = {}\n",
    "data_state['California'] = 8\n",
    "data_state['Nevada'] = 1\n",
    "data_state['Utah'] = 1\n",
    "data_state['Texas'] = 6\n",
    "data_state['Illinois'] = 1\n",
    "data_state['Florida'] = 7\n",
    "data_state['Virginia'] = 1\n",
    "data_state['Washington'] = 2\n",
    "data_state['Pennsylvania'] = 1\n",
    "data_state['New Jersey'] = 1\n",
    "data_state['New York'] = 9\n",
    "\n",
    "\n",
    "# Define state names and their coordinates (You need to provide this data)\n",
    "states = {\n",
    "    \"Alabama\": {\"lat\": 32.806671, \"lon\": -86.791130},\n",
    "    \"Alaska\": {\"lat\": 61.370716, \"lon\": -152.404419},\n",
    "    \"Arizona\": {\"lat\": 33.729759, \"lon\": -111.431221},\n",
    "    \"Arkansas\": {\"lat\": 34.969704, \"lon\": -92.373123},\n",
    "    \"California\": {\"lat\": 36.778261, \"lon\": -119.417932},\n",
    "    \"Colorado\": {\"lat\": 39.550051, \"lon\": -105.782067},\n",
    "    \"Connecticut\": {\"lat\": 41.597782, \"lon\": -72.755371},\n",
    "    \"Delaware\": {\"lat\": 39.318523, \"lon\": -75.507141},\n",
    "    \"Florida\": {\"lat\": 27.766279, \"lon\": -81.686783},\n",
    "    \"Georgia\": {\"lat\": 33.040619, \"lon\": -83.643074},\n",
    "    \"Hawaii\": {\"lat\": 21.094318, \"lon\": -157.498337},\n",
    "    \"Idaho\": {\"lat\": 44.240459, \"lon\": -114.478828},\n",
    "    \"Illinois\": {\"lat\": 40.349457, \"lon\": -88.986137},\n",
    "    \"Indiana\": {\"lat\": 39.849426, \"lon\": -86.258278},\n",
    "    \"Iowa\": {\"lat\": 42.011539, \"lon\": -93.210526},\n",
    "    \"Kansas\": {\"lat\": 38.526600, \"lon\": -96.726486},\n",
    "    \"Kentucky\": {\"lat\": 37.668140, \"lon\": -84.670067},\n",
    "    \"Louisiana\": {\"lat\": 31.169546, \"lon\": -91.867805},\n",
    "    \"Maine\": {\"lat\": 44.693947, \"lon\": -69.381927},\n",
    "    \"Maryland\": {\"lat\": 39.063946, \"lon\": -76.802101},\n",
    "    \"Massachusetts\": {\"lat\": 42.230171, \"lon\": -71.530106},\n",
    "    \"Michigan\": {\"lat\": 43.326618, \"lon\": -84.536095},\n",
    "    \"Minnesota\": {\"lat\": 45.694454, \"lon\": -93.900192},\n",
    "    \"Mississippi\": {\"lat\": 32.741646, \"lon\": -89.678696},\n",
    "    \"Missouri\": {\"lat\": 38.456085, \"lon\": -92.288368},\n",
    "    \"Montana\": {\"lat\": 46.921925, \"lon\": -110.454353},\n",
    "    \"Nebraska\": {\"lat\": 41.125370, \"lon\": -98.268082},\n",
    "    \"Nevada\": {\"lat\": 38.313515, \"lon\": -117.055374},\n",
    "    \"New Hampshire\": {\"lat\": 43.452492, \"lon\": -71.563896},\n",
    "    \"New Jersey\": {\"lat\": 40.298904, \"lon\": -74.521011},\n",
    "    \"New Mexico\": {\"lat\": 34.840515, \"lon\": -106.248482},\n",
    "    \"New York\": {\"lat\": 42.165726, \"lon\": -74.948051},\n",
    "    \"North Carolina\": {\"lat\": 35.630066, \"lon\": -79.806419},\n",
    "    \"North Dakota\": {\"lat\": 47.528912, \"lon\": -99.784012},\n",
    "    \"Ohio\": {\"lat\": 40.388783, \"lon\": -82.764915},\n",
    "    \"Oklahoma\": {\"lat\": 35.565342, \"lon\": -96.928917},\n",
    "    \"Oregon\": {\"lat\": 44.572021, \"lon\": -122.070938},\n",
    "    \"Pennsylvania\": {\"lat\": 40.590752, \"lon\": -77.209755},\n",
    "    \"Rhode Island\": {\"lat\": 41.680893, \"lon\": -71.511780},\n",
    "    \"South Carolina\": {\"lat\": 33.856892, \"lon\": -80.945007},\n",
    "    \"South Dakota\": {\"lat\": 44.299782, \"lon\": -99.438828},\n",
    "    \"Tennessee\": {\"lat\": 35.747845, \"lon\": -86.692345},\n",
    "    \"Texas\": {\"lat\": 31.054487, \"lon\": -97.563461},\n",
    "    \"Utah\": {\"lat\": 40.150032, \"lon\": -111.862434},\n",
    "    \"Vermont\": {\"lat\": 44.045876, \"lon\": -72.710686},\n",
    "    \"Virginia\": {\"lat\": 37.769337, \"lon\": -78.169968},\n",
    "    \"Washington\": {\"lat\": 47.400902, \"lon\": -121.490494},\n",
    "    \"West Virginia\": {\"lat\": 38.491226, \"lon\": -80.954570},\n",
    "    \"Wisconsin\": {\"lat\": 44.268543, \"lon\": -89.616508},\n",
    "    \"Wyoming\": {\"lat\": 42.755966, \"lon\": -107.302490}\n",
    "}\n",
    "\n",
    "geo_ = Geo(init_opts=opts.InitOpts(theme=ThemeType.LIGHT))\n",
    "geo_.add_schema(maptype=\"美国\")\n",
    "\n",
    "# # Add coordinates for states\n",
    "# for state, coord in states.items():\n",
    "#     geo_.add_coordinate(name=state, longitude=coord[\"lon\"], latitude=coord[\"lat\"])\n",
    "#     if data_state.get(state) != None:\n",
    "#         geo_.add(\"State\", [(state, data_state[state])], type_=ChartType.EFFECT_SCATTER)\n",
    "#     else:\n",
    "#         geo_.add(\"State\", [(state, 0)], type_=ChartType.EFFECT_SCATTER)\n",
    "\n",
    "# Add coordinates and heatmap data for stores\n",
    "for i in range(len(data_unique)):\n",
    "    # print(data_unique['store_address'][i], data_unique['rating_count'][i])\n",
    "    print(data_unique['longitude'][i], data_unique['latitude'][i])\n",
    "    geo_.add_coordinate(name=data_unique['store_address'][i], longitude=data_unique['longitude'][i], latitude=data_unique['latitude'][i])\n",
    "    geo_.add(\"McDonald Store\", [(data_unique['store_address'][i], data_unique['rating'][i])], type_=ChartType.EFFECT_SCATTER)\n",
    "\n",
    "# Set labels and title\n",
    "geo_.set_series_opts(\n",
    "    label_opts=opts.LabelOpts(is_show=False),  # Show labels for states\n",
    ")\n",
    "geo_.set_global_opts(\n",
    "    title_opts=opts.TitleOpts(title=\"McDonald's Locations in USA\"),\n",
    "    visualmap_opts=opts.VisualMapOpts(min_=2,max_=4,is_piecewise=True),\n",
    "     toolbox_opts=opts.ToolboxOpts()\n",
    "    \n",
    ")\n",
    "\n",
    "# Render the output file\n",
    "geo_output_file = '../res/lab3/mcdonalds_locations_geo.html'\n",
    "geo_.render(geo_output_file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T05:12:02.371763463Z",
     "start_time": "2023-12-20T05:12:02.331311260Z"
    }
   },
   "id": "4445d929e8d6769d"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "\"/home/ByteBloom/Visualization/res/lab3/graph for McDonald's.html\""
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pie chart to show the average rating of selected McDonald's Store\n",
    "\n",
    "\n",
    "# Prepare the data for the pie chart\n",
    "rating_levels = ['Poor', 'Fair', 'Normal', 'Good', 'Excellent']\n",
    "rating_counts = [len(data_unique[(data_unique['rating'] >= 2.0) & (data_unique['rating'] < 2.4)]),\n",
    "                 len(data_unique[(data_unique['rating'] >= 2.4) & (data_unique['rating'] < 2.8)]),\n",
    "                 len(data_unique[(data_unique['rating'] >= 2.8) & (data_unique['rating'] < 3.2)]),\n",
    "                 len(data_unique[(data_unique['rating'] >= 3.2) & (data_unique['rating'] < 3.6)]),\n",
    "                 len(data_unique[(data_unique['rating'] >= 3.6) & (data_unique['rating'] <= 4.0)])]\n",
    "\n",
    "page = Page(layout=Page.SimplePageLayout)\n",
    "\n",
    "# Create the rosetype pie chart\n",
    "pie = (\n",
    "    Pie(init_opts=opts.InitOpts(theme=ThemeType.LIGHT))\n",
    "    .add(\n",
    "        series_name=\"Average Rating Level\",\n",
    "        data_pair=[list(z) for z in zip(rating_levels, rating_counts)],\n",
    "        radius=[\"20%\", \"75%\"],\n",
    "        center=[\"50%\", \"50%\"],\n",
    "        rosetype=\"area\",\n",
    "        label_opts=opts.LabelOpts(formatter=\"{b}: {c}\"),\n",
    "    )\n",
    "    .set_global_opts(title_opts=opts.TitleOpts(title=\"Average Rating Level\"),\n",
    "                     toolbox_opts=opts.ToolboxOpts(),)\n",
    "    .set_series_opts(label_opts=opts.LabelOpts(formatter=\"{b}: {c}\"),)\n",
    ")\n",
    "\n",
    "# create a pie chart to show the number of stores in each state\n",
    "state_counts = data_unique['state'].value_counts()\n",
    "# divide the state_counts index and value to 2 lists\n",
    "state_counts_index = state_counts.index.tolist()\n",
    "state_counts_values = state_counts.values.tolist()\n",
    "# Create the pie chart\n",
    "pie_state = (\n",
    "    Pie(init_opts=opts.InitOpts(theme=ThemeType.LIGHT))\n",
    "    .add(\n",
    "        series_name=\"Number of Stores by State\",\n",
    "        data_pair=[list(z) for z in zip(state_counts_index, state_counts_values)],\n",
    "        radius=[\"20%\", \"75%\"],\n",
    "        center=[\"50%\", \"50%\"],\n",
    "        rosetype=\"area\",\n",
    "        label_opts=opts.LabelOpts(formatter=\"{b}: {c}\"),\n",
    "    )\n",
    "    .set_global_opts(title_opts=opts.TitleOpts(title=\"Number of Stores by State\"),\n",
    "                      legend_opts=opts.LegendOpts(type_=\"scroll\", pos_right=\"0%\", orient=\"vertical\"),\n",
    "                     toolbox_opts=opts.ToolboxOpts())\n",
    "    .set_series_opts(label_opts=opts.LabelOpts(formatter=\"{b}: {c}\"))\n",
    ")\n",
    "\n",
    "page.add(geo_)\n",
    "page.add(parallel)\n",
    "page.add(pie)\n",
    "page.add(pie_state)\n",
    "\n",
    "\n",
    "# Render the chart to an HTML file\n",
    "pie.render(\"../res/lab3/pie.html\")\n",
    "pie_state.render(\"../res/lab3/pie_state.html\")\n",
    "page.render(\"../res/lab3/graph for McDonald's.html\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T05:12:08.147199184Z",
     "start_time": "2023-12-20T05:12:08.125095506Z"
    }
   },
   "id": "c945a30c17c2904e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Word Cloud 1 to show the most frequent words in the reviews\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load the data from the CSV file\n",
    "# Load the data\n",
    "data = pd.read_csv('../dataset/McDonald/McDonald_s_Reviews.csv')\n",
    "\n",
    "# Function to clean and split text into words\n",
    "def clean_and_split(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # Remove non-alphabetic characters and split into words\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "    return words\n",
    "# Define a function to clean the 'rating' column and convert to numeric\n",
    "def clean_rating(rating):\n",
    "    if rating == \"1 star\":\n",
    "        return float(rating.replace(' star', ''))\n",
    "    else:\n",
    "        return float(rating.replace(' stars', ''))\n",
    "\n",
    "# Apply the function to the 'rating' column\n",
    "data['rating'] = data['rating'].apply(clean_rating)\n",
    "\n",
    "\n",
    "# Extract the 'review' column\n",
    "reviews = data['review']\n",
    "\n",
    "# regard rating > 3 as positive, rating < 3 as negative\n",
    "reviews_positive = reviews[data['rating'] > 3]\n",
    "reviews_negative = reviews[data['rating'] < 2]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "\n",
    "# Apply TF-IDF to the reviews\n",
    "tfidf_matrix = vectorizer.fit_transform(reviews.fillna(\"\"))  # Filling missing values with empty strings\n",
    "tfidf_matrix_positive = vectorizer.fit_transform(reviews_positive.fillna(\"\"))  # Filling missing values with empty strings\n",
    "tfidf_matrix_negative = vectorizer.fit_transform(reviews_negative.fillna(\"\"))  # Filling missing values with empty strings\n",
    "\n",
    "# Summarize the TF-IDF scores for each word\n",
    "word_scores = tfidf_matrix.sum(axis=0)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "word_freq_tfidf = {words[i]: word_scores[0, i] for i in range(len(words))}\n",
    "\n",
    "# Summarize the TF-IDF scores for each positive word\n",
    "word_scores_positive = tfidf_matrix_positive.sum(axis=0)\n",
    "words_positive = vectorizer.get_feature_names_out()\n",
    "word_freq_tfidf_positive = {words_positive[i]: word_scores_positive[0, i] for i in range(len(words_positive))}\n",
    "# Summarize the TF-IDF scores for each negative word\n",
    "word_scores_negative = tfidf_matrix_negative.sum(axis=0)\n",
    "words_negative = vectorizer.get_feature_names_out()\n",
    "word_freq_tfidf_negative = {words_negative[i]: word_scores_negative[0, i] for i in range(len(words_negative))}\n",
    "\n",
    "# Generate word cloud using the TF-IDF scores\n",
    "wordcloud_tfidf = WordCloud(width=800, height=600, background_color='white').generate_from_frequencies(word_freq_tfidf)\n",
    "wordcloud_tfidf_positive = WordCloud(width=800, height=600, background_color='white').generate_from_frequencies(word_freq_tfidf_positive)\n",
    "wordcloud_tfidf_negative = WordCloud(width=800, height=600, background_color='white').generate_from_frequencies(word_freq_tfidf_negative)\n",
    "\n",
    "# Plotting the Word Cloud\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud_tfidf, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "\n",
    "# Save the word cloud image\n",
    "wordcloud_image_path_tfidf = '../res/wordcloud_tfidf.png'\n",
    "plt.savefig(wordcloud_image_path_tfidf)\n",
    "\n",
    "# Plotting the Word Cloud positive\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud_tfidf_positive, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "# save the word cloud image\n",
    "wordcloud_image_path_tfidf_positive = '../res/wordcloud_tfidf_positive.png'\n",
    "plt.savefig(wordcloud_image_path_tfidf_positive)\n",
    "\n",
    "# Plotting the Word Cloud negative\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud_tfidf_negative, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "\n",
    "# Save the word cloud image\n",
    "wordcloud_image_path_tfidf_negative = '../res/wordcloud_tfidf_negative.png'\n",
    "plt.savefig(wordcloud_image_path_tfidf_negative)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "999967387e9c3f8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Word Cloud 2 in 300 adj words\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../dataset/McDonald/McDonald_s_Reviews.csv')\n",
    "\n",
    "# Define a function to clean the 'rating' column and convert to numeric\n",
    "def clean_rating(rating):\n",
    "    if \"1 star\" in rating:\n",
    "        return 1\n",
    "    else:\n",
    "        return float(rating.replace(' stars', ''))\n",
    "\n",
    "# Apply the function to the 'rating' column\n",
    "data['rating'] = data['rating'].apply(clean_rating)\n",
    "\n",
    "# Filter reviews based on rating\n",
    "reviews_positive = data[data['rating'] > 3]['review'].fillna(\"\")\n",
    "reviews_negative = data[data['rating'] < 3]['review'].fillna(\"\")\n",
    "\n",
    "# List of common adjectives in restaurant reviews\n",
    "common_adjectives = [\n",
    "    \"good\", \"great\", \"bad\", \"delicious\", \"tasty\", \"poor\", \"excellent\", \"terrible\", \"nice\", \"friendly\", \n",
    "    \"dirty\", \"clean\", \"slow\", \"fast\", \"fresh\", \"old\", \"hot\", \"cold\", \"spicy\", \"sweet\", \"savory\", \"rich\",\n",
    "    \"creamy\", \"crispy\", \"juicy\", \"sour\", \"bitter\", \"yummy\", \"flavorful\", \"tangy\", \"zesty\", \"smoky\", \n",
    "    \"grilled\", \"baked\", \"roasted\", \"fried\", \"steamed\", \"raw\", \"crunchy\", \"soft\", \"hard\", \"dense\", \n",
    "    \"light\", \"hearty\", \"healthy\", \"unhealthy\", \"salty\", \"bland\", \"seasoned\", \"spiced\", \"zippy\", \"buttery\", \n",
    "    \"cheesy\", \"chocolaty\", \"nutty\", \"fruity\", \"meaty\", \"vegetarian\", \"vegan\", \"gluten-free\", \"organic\", \n",
    "    \"local\", \"exotic\", \"traditional\", \"innovative\", \"creative\", \"unique\", \"classic\", \"modern\", \"fancy\", \n",
    "    \"casual\", \"cozy\", \"comfortable\", \"elegant\", \"stylish\", \"busy\", \"quiet\", \"loud\", \"crowded\", \"empty\", \n",
    "    \"spacious\", \"compact\", \"intimate\", \"romantic\", \"family-friendly\", \"welcoming\", \"unfriendly\", \"rude\", \n",
    "    \"polite\", \"attentive\", \"neglectful\", \"helpful\", \"unhelpful\", \"knowledgeable\", \"inexperienced\", \n",
    "    \"professional\", \"amateurish\", \"quick\", \"lengthy\", \"timely\", \"late\", \"early\", \"expensive\", \"cheap\", \n",
    "    \"affordable\", \"overpriced\", \"worthwhile\", \"valuable\", \"pricey\", \"economical\", \"luxurious\", \"opulent\", \n",
    "    \"lavish\", \"simple\", \"plain\", \"ornate\", \"decorative\", \"colorful\", \"drab\", \"bright\", \"dark\", \"warm\", \n",
    "    \"cool\", \"airy\", \"stuffy\", \"smoky\", \"fragrant\", \"aromatic\", \"odorous\", \"scented\", \"musty\", \"fresh\", \n",
    "    \"stale\", \"moist\", \"dry\", \"humid\", \"damp\", \"soggy\", \"wet\", \"arid\", \"crispy\", \"chewy\", \"tender\", \n",
    "    \"tough\", \"rubbery\", \"succulent\", \"lean\", \"fatty\", \"greasy\", \"oily\", \"smooth\", \"gritty\", \"fine\", \n",
    "    \"coarse\", \"thick\", \"thin\", \"hefty\", \"lightweight\", \"substantial\", \"insubstantial\", \"filling\", \n",
    "    \"satisfying\", \"unsatisfying\", \"appetizing\", \"unappetizing\", \"delectable\", \"disgusting\", \"enjoyable\", \n",
    "    \"unenjoyable\", \"pleasant\", \"unpleasant\", \"tantalizing\", \"uninviting\", \"inviting\", \"appealing\", \n",
    "    \"unappealing\", \"enticing\", \"repulsive\", \"luscious\", \"revolting\", \"enticing\", \"alluring\", \"off-putting\", \n",
    "    \"gorgeous\", \"unattractive\", \"stunning\", \"dreary\", \"vibrant\", \"dull\", \"radiant\", \"gloomy\", \"shiny\", \n",
    "    \"matte\", \"glossy\", \"polished\", \"unpolished\", \"sleek\", \"rough\", \"ragged\", \"neat\", \"messy\", \"orderly\", \n",
    "    \"chaotic\", \"organized\", \"disorganized\", \"compact\", \"spacious\", \"cramped\", \"roomy\", \"airy\", \"claustrophobic\", \n",
    "    \"open\", \"closed\", \"expansive\", \"narrow\", \"broad\", \"wide\", \"constricted\", \"expansive\", \"capacious\", \n",
    "    \"generous\", \"stingy\", \"ample\", \"limited\", \"abundant\", \"scarce\", \"plentiful\", \"meager\", \"adequate\", \n",
    "    \"insufficient\", \"sufficient\", \"lacking\", \"complete\", \"incomplete\", \"whole\", \"partial\", \"comprehensive\", \n",
    "    \"limited\", \"detailed\", \"sketchy\", \"specific\", \"general\", \"particular\", \"vague\", \"precise\", \"inaccurate\", \n",
    "    \"accurate\", \"true\", \"false\", \"real\", \"fake\", \"authentic\", \"imitation\", \"genuine\", \"artificial\", \n",
    "    \"natural\", \"synthetic\", \"manmade\", \"handmade\", \"machine-made\", \"crafted\", \"manufactured\", \"produced\", \n",
    "    \"created\", \"designed\", \"planned\", \"improvised\", \"intentional\", \"accidental\", \"deliberate\", \"spontaneous\", \n",
    "    \"calculated\", \"unplanned\", \"organized\", \"disordered\", \"systematic\", \"haphazard\", \"methodical\", \"random\", \n",
    "    \"consistent\", \"inconsistent\", \"uniform\", \"varied\", \"similar\", \"different\", \"alike\", \"unlike\", \n",
    "    \"comparable\", \"incomparable\", \"equivalent\", \"unequal\", \"equal\", \"superior\", \"inferior\", \"better\", \n",
    "    \"worse\", \"improved\", \"degraded\", \"enhanced\", \"diminished\", \"upgraded\", \"downgraded\", \"advanced\", \n",
    "    \"primitive\", \"modern\", \"antique\", \"contemporary\", \"historic\", \"new\", \"old\", \"fresh\", \"stale\", \n",
    "    \"recent\", \"ancient\", \"current\", \"outdated\", \"latest\", \"earliest\", \"first\", \"last\", \"initial\", \n",
    "    \"final\", \"beginning\", \"end\", \"start\", \"finish\", \"opening\", \"closing\", \"launch\", \"conclusion\", \n",
    "    \"introduction\", \"completion\", \"inception\", \"termination\", \"genesis\", \"demise\", \"birth\", \"death\", \n",
    "    \"origin\", \"conclusion\", \"foundation\", \"destruction\", \"creation\", \"annihilation\", \"formation\", \"dissolution\", \n",
    "    \"assembly\", \"disassembly\", \"construction\", \"deconstruction\", \"building\", \"demolition\", \"erecting\", \"razing\", \n",
    "    \"raising\", \"lowering\", \"elevating\", \"depressing\", \"ascending\", \"descending\", \"climbing\",\n",
    "    \"descending\", \"climbing\", \"falling\", \"rising\", \"dropping\", \"plummeting\", \"soaring\", \"plunging\", \"skyrocketing\",]\n",
    "\n",
    "# Function to filter text for these adjectives\n",
    "def filter_adjectives(text):\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "    return ' '.join(word for word in words if word in common_adjectives)\n",
    "\n",
    "# Apply adjective filtering to the reviews\n",
    "reviews_positive_adj = reviews_positive.apply(filter_adjectives)\n",
    "reviews_negative_adj = reviews_negative.apply(filter_adjectives)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "\n",
    "# Apply TF-IDF to the adjective reviews\n",
    "tfidf_matrix_positive = vectorizer.fit_transform(reviews_positive_adj)\n",
    "tfidf_matrix_negative = vectorizer.fit_transform(reviews_negative_adj)\n",
    "\n",
    "# Summarize the TF-IDF scores for positive and negative adjectives\n",
    "word_scores_positive = tfidf_matrix_positive.sum(axis=0)\n",
    "word_scores_negative = tfidf_matrix_negative.sum(axis=0)\n",
    "\n",
    "words_positive = vectorizer.get_feature_names_out()\n",
    "words_negative = vectorizer.get_feature_names_out()\n",
    "\n",
    "word_freq_tfidf_positive = {words_positive[i]: word_scores_positive[0, i] for i in range(len(words_positive))}\n",
    "word_freq_tfidf_negative = {words_negative[i]: word_scores_negative[0, i] for i in range(len(words_negative))}\n",
    "\n",
    "# Generate word clouds\n",
    "wordcloud_tfidf_positive = WordCloud(width=800, height=600, background_color='white').generate_from_frequencies(word_freq_tfidf_positive)\n",
    "wordcloud_tfidf_negative = WordCloud(width=800, height=600, background_color='white').generate_from_frequencies(word_freq_tfidf_negative)\n",
    "\n",
    "# Plotting the Word Clouds\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud_tfidf_positive, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig('../res/wordcloud_tfidf_positive.png')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud_tfidf_negative, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig('../res/wordcloud_tfidf_negative.png')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a01ba6330b999c04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b8934ce6599ec582"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../dataset/McDonald/McDonald_s_Reviews.csv')\n",
    "\n",
    "# Define a function to clean the 'rating' column and convert to numeric\n",
    "def clean_rating(rating):\n",
    "    if \"1 star\" in rating:\n",
    "        return 1\n",
    "    elif \"2 stars\" in rating:\n",
    "        return 2\n",
    "    elif \"3 stars\" in rating:\n",
    "        return 3\n",
    "    elif \"4 stars\" in rating:\n",
    "        return 4\n",
    "    elif \"5 stars\" in rating:\n",
    "        return 5\n",
    "    else:\n",
    "        return 0  # For any non-standard ratings\n",
    "\n",
    "# Apply the function to the 'rating' column\n",
    "data['numeric_rating'] = data['rating'].apply(clean_rating)\n",
    "\n",
    "# Consider ratings of 4 or 5 stars as positive (1), and the rest as negative (0)\n",
    "data['sentiment'] = data['numeric_rating'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = data['review'].fillna(\" \")  # Filling missing reviews with a blank space\n",
    "y = data['sentiment']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test_vectorized)\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# save the model\n",
    "import pickle\n",
    "pickle.dump(clf, open(\"../res/lab3/mcdonalds_sentiment_model.pkl\", 'wb'))\n",
    "\n",
    "\n",
    "# load the model to predict\n",
    "model = pickle.load(open(\"../res/lab3/mcdonalds_sentiment_model.pkl\", 'rb'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f04f217b2453aee7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
